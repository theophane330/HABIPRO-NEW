from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import PyPDF2
import io
from typing import Optional
import uvicorn
import google.generativeai as genai

app = FastAPI(title="API Question-R√©ponse PDF avec Google Gemini")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# üîë CONFIGURATION : Mettez votre cl√© API ici
# ============================================
GEMINI_API_KEY = "AIzaSyCoACjAyiZFpqWJkZfazfzzXAMpI3wRuXI"
# Obtenez votre cl√© sur : https://makersuite.google.com/app/apikey
# ============================================

# Initialiser Gemini
try:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('models/gemini-2.5-flash')
except Exception as e:
    print(f"‚ö†Ô∏è Erreur d'initialisation: {e}")
    print("V√©rifiez votre cl√© API Gemini")

# Stockage temporaire du contenu du PDF
pdf_content = {"text": "", "uploaded": False, "filename": ""}

class Question(BaseModel):
    model_config = {"protected_namespaces": ()}
    
    question: str
    max_tokens: Optional[int] = 1024
    model_name: Optional[str] = "models/gemini-2.5-flash"  # models/gemini-2.5-flash, models/gemini-2.5-pro

class Answer(BaseModel):
    question: str
    answer: str
    source: str
    model: str

def extract_text_from_pdf(pdf_file):
    """Extrait le texte d'un fichier PDF"""
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page_num, page in enumerate(pdf_reader.pages, 1):
            page_text = page.extract_text()
            text += f"\n--- Page {page_num} ---\n{page_text}\n"
        return text
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Erreur lors de l'extraction du PDF: {str(e)}")

def split_document_into_chunks(text: str, max_chunk_size: int = 4000) -> list:
    """Divise le document en morceaux plus petits"""
    words = text.split()
    chunks = []
    current_chunk = []
    current_size = 0
    
    for word in words:
        current_size += len(word) + 1
        if current_size > max_chunk_size:
            chunks.append(' '.join(current_chunk))
            current_chunk = [word]
            current_size = len(word)
        else:
            current_chunk.append(word)
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

def get_answer_from_gemini(document_text: str, question: str, max_tokens: int = 1024, model_name: str = "models/gemini-2.5-flash") -> str:
    """Utilise Google Gemini pour g√©n√©rer une r√©ponse bas√©e sur le document avec syst√®me de fallback robuste"""
    try:
        model = genai.GenerativeModel(model_name)
        
        # Configuration de g√©n√©ration optimis√©e
        generation_config = {
            "max_output_tokens": max_tokens,
            "temperature": 0.3,
            "top_p": 0.95,
            "top_k": 40,
        }
        
        # Configuration de s√©curit√© minimale
        safety_settings = {
            genai.types.HarmCategory.HARM_CATEGORY_HARASSMENT: genai.types.HarmBlockThreshold.BLOCK_NONE,
            genai.types.HarmCategory.HARM_CATEGORY_HATE_SPEECH: genai.types.HarmBlockThreshold.BLOCK_NONE,
            genai.types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: genai.types.HarmBlockThreshold.BLOCK_NONE,
            genai.types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: genai.types.HarmBlockThreshold.BLOCK_NONE,
        }

        # ===== STRAT√âGIE 1 : Essayer avec le document complet =====
        try:
            prompt = f"""Analyse le document suivant et r√©ponds √† la question de mani√®re factuelle et objective.

DOCUMENT:
{document_text}

QUESTION: {question}

INSTRUCTIONS:
- R√©ponds uniquement en te basant sur les informations du document
- Sois pr√©cis et factuel
- Si l'information n'est pas dans le document, indique-le clairement
- Structure ta r√©ponse de mani√®re claire avec des points cl√©s si n√©cessaire

R√âPONSE:"""

            response = model.generate_content(
                prompt,
                generation_config=generation_config,
                safety_settings=safety_settings
            )
            
            if hasattr(response, 'text') and response.text:
                return response.text
            
            print("Strat√©gie 1 : Pas de texte dans la r√©ponse, passage √† la strat√©gie 2")
        
        except Exception as e:
            print(f"Strat√©gie 1 √©chou√©e: {e}")
        
        # ===== STRAT√âGIE 2 : Prompt plus neutre et court =====
        try:
            # Limiter la taille du document si trop long
            doc_excerpt = document_text[:8000] if len(document_text) > 8000 else document_text
            
            prompt = f"""En tant qu'assistant d'analyse documentaire, r√©ponds √† cette question bas√©e sur le document.

Question: {question}

Document (extrait):
{doc_excerpt}

R√©ponds de mani√®re concise et factuelle."""

            response = model.generate_content(
                prompt,
                generation_config=generation_config,
                safety_settings=safety_settings
            )
            
            if hasattr(response, 'text') and response.text:
                return response.text
            
            print("Strat√©gie 2 : Pas de texte dans la r√©ponse, passage √† la strat√©gie 3")
        
        except Exception as e:
            print(f"Strat√©gie 2 √©chou√©e: {e}")
        
        # ===== STRAT√âGIE 3 : Diviser en chunks et analyser =====
        try:
            chunks = split_document_into_chunks(document_text, max_chunk_size=3000)
            
            chunk_responses = []
            for i, chunk in enumerate(chunks[:3]):  # Limiter √† 3 chunks pour √©viter les timeouts
                try:
                    prompt = f"""Analyse ce segment de document et r√©ponds √† la question si l'information est pr√©sente.

Segment {i+1}/{min(len(chunks), 3)}:
{chunk}

Question: {question}

Si ce segment contient des informations pertinentes, r√©ponds. Sinon, r√©ponds "Non pertinent"."""

                    response = model.generate_content(
                        prompt,
                        generation_config=generation_config,
                        safety_settings=safety_settings
                    )
                    
                    if hasattr(response, 'text') and response.text:
                        response_text = response.text.strip()
                        if response_text and "Non pertinent" not in response_text:
                            chunk_responses.append(response_text)
                
                except Exception as chunk_error:
                    print(f"Erreur chunk {i}: {chunk_error}")
                    continue
            
            if chunk_responses:
                # Si on a plusieurs r√©ponses, essayer de les synth√©tiser
                if len(chunk_responses) > 1:
                    try:
                        synthesis_prompt = f"""Synth√©tise ces informations en une seule r√©ponse coh√©rente pour la question: "{question}"

Informations collect√©es:
{chr(10).join([f"{i+1}. {resp}" for i, resp in enumerate(chunk_responses)])}

Fournis une r√©ponse synth√©tique et claire."""

                        final_response = model.generate_content(
                            synthesis_prompt,
                            generation_config=generation_config,
                            safety_settings=safety_settings
                        )
                        
                        if hasattr(final_response, 'text') and final_response.text:
                            return final_response.text
                    except:
                        pass
                
                # Retourner les r√©ponses combin√©es
                return "\n\n".join(chunk_responses)
            
            print("Strat√©gie 3 : Aucune r√©ponse pertinente dans les chunks, passage √† la strat√©gie 4")
        
        except Exception as e:
            print(f"Strat√©gie 3 √©chou√©e: {e}")
        
        # ===== STRAT√âGIE 4 : Analyse g√©n√©rique structur√©e =====
        try:
            # Extraire des informations de base du document
            doc_preview = document_text[:2000].strip()
            word_count = len(document_text.split())
            char_count = len(document_text)
            
            prompt = f"""Voici les premi√®res lignes d'un document:

{doc_preview}

Le document complet contient environ {word_count} mots et {char_count} caract√®res.

Question de l'utilisateur: {question}

Fournis une r√©ponse bas√©e sur ce que tu peux d√©duire du contenu visible."""

            response = model.generate_content(
                prompt,
                generation_config=generation_config,
                safety_settings=safety_settings
            )
            
            if hasattr(response, 'text') and response.text:
                return response.text
        
        except Exception as e:
            print(f"Strat√©gie 4 √©chou√©e: {e}")
        
        # ===== STRAT√âGIE 5 : R√©ponse de secours structur√©e =====
        # Extraire quelques informations basiques du document
        lines = document_text.split('\n')
        non_empty_lines = [line.strip() for line in lines if line.strip()][:10]
        
        fallback_response = f"""Bas√© sur le document analys√©, voici ce que je peux vous dire :

üìÑ **Informations g√©n√©rales :**
- Le document contient environ {len(document_text.split())} mots
- Structure : {len(lines)} lignes de texte
- Premi√®res lignes visibles : "{non_empty_lines[0][:100]}..."

‚ùì **Concernant votre question :** "{question}"

Pour obtenir une r√©ponse plus pr√©cise, essayez de :

1. **Poser une question plus sp√©cifique** sur une section pr√©cise
2. **Demander des informations cibl√©es** (dates, noms, chiffres, d√©finitions)
3. **Reformuler la question** de mani√®re plus directe

**Exemples de questions qui fonctionnent bien :**
- "Quelles sont les dates mentionn√©es ?"
- "Quels sont les chiffres cl√©s du document ?"
- "De quoi parle la premi√®re page ?"
- "Quelle est la d√©finition de [terme] dans le document ?"
- "Qui sont les personnes ou organisations cit√©es ?"

Je reste √† votre disposition pour r√©pondre √† une question plus cibl√©e ! üòä"""

        return fallback_response
    
    except Exception as e:
        # Derni√®re ligne de d√©fense : toujours retourner quelque chose d'utile
        return f"""Une erreur technique s'est produite lors de l'analyse ({str(e)}).

Cependant, le document est bien charg√© et contient {len(document_text)} caract√®res.

**Suggestions :**
1. R√©essayez avec une question plus simple
2. Posez une question sur un aspect sp√©cifique du document
3. Demandez un r√©sum√© g√©n√©ral : "R√©sume bri√®vement ce document"

Le syst√®me est op√©rationnel et pr√™t √† r√©pondre √† vos questions ! üëç"""

@app.post("/upload-pdf/")
async def upload_pdf(file: UploadFile = File(...)):
    """Upload un fichier PDF et extrait son contenu"""
    if not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Le fichier doit √™tre un PDF")
    
    try:
        contents = await file.read()
        pdf_file = io.BytesIO(contents)
        text = extract_text_from_pdf(pdf_file)
        
        if not text.strip():
            raise HTTPException(status_code=400, detail="Le PDF ne contient pas de texte extractible")
        
        pdf_content["text"] = text
        pdf_content["uploaded"] = True
        pdf_content["filename"] = file.filename
        
        return {
            "message": "PDF upload√© avec succ√®s",
            "filename": file.filename,
            "characters": len(text),
            "words": len(text.split()),
            "estimated_tokens": len(text) // 4
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}")

@app.post("/ask/", response_model=Answer)
async def ask_question(question: Question):
    """Pose une question sur le PDF upload√© en utilisant Gemini"""
    if not pdf_content["uploaded"]:
        raise HTTPException(status_code=400, detail="Veuillez d'abord uploader un PDF")
    
    if not question.question.strip():
        raise HTTPException(status_code=400, detail="La question ne peut pas √™tre vide")
    
    # Cette route ne l√®ve JAMAIS d'exception, elle retourne toujours une r√©ponse
    try:
        answer = get_answer_from_gemini(
            pdf_content["text"], 
            question.question,
            question.max_tokens,
            question.model_name
        )
        
        # V√©rifier que la r√©ponse n'est pas vide
        if not answer or answer.strip() == "":
            answer = f"""Je n'ai pas pu g√©n√©rer une r√©ponse sp√©cifique √† votre question "{question.question}".

Le document est charg√© et contient {len(pdf_content['text'])} caract√®res.

Essayez de reformuler votre question de mani√®re plus pr√©cise ou plus simple."""
        
        return Answer(
            question=question.question,
            answer=answer,
            source=pdf_content["filename"],
            model=question.model_name
        )
    
    except Exception as e:
        # M√™me en cas d'erreur, retourner une r√©ponse valide et utile
        print(f"Erreur dans ask_question: {e}")
        
        return Answer(
            question=question.question,
            answer=f"""Une erreur technique s'est produite, mais le syst√®me reste op√©rationnel.

üìÑ Document charg√© : {pdf_content["filename"]}
üìä Taille : {len(pdf_content['text'])} caract√®res

**Suggestions :**
- R√©essayez avec une question plus simple
- Posez une question sur un aspect sp√©cifique
- Demandez "De quoi parle ce document ?"

Erreur technique : {str(e)[:200]}""",
            source=pdf_content["filename"],
            model=question.model_name
        )

@app.get("/status/")
async def get_status():
    """V√©rifie le statut de l'API et si un PDF est charg√©"""
    api_key_configured = GEMINI_API_KEY != "votre_cl√©_api_gemini_ici"
    
    return {
        "api_status": "operational",
        "gemini_api_configured": api_key_configured,
        "pdf_uploaded": pdf_content["uploaded"],
        "filename": pdf_content["filename"] if pdf_content["uploaded"] else None,
        "text_length": len(pdf_content["text"]) if pdf_content["uploaded"] else 0,
        "word_count": len(pdf_content["text"].split()) if pdf_content["uploaded"] else 0,
        "estimated_tokens": len(pdf_content["text"]) // 4 if pdf_content["uploaded"] else 0,
        "available_models": ["models/gemini-2.5-flash", "models/gemini-2.5-pro", "models/gemini-2.0-flash"]
    }

@app.delete("/clear/")
async def clear_pdf():
    """Efface le PDF en m√©moire"""
    pdf_content["text"] = ""
    pdf_content["uploaded"] = False
    pdf_content["filename"] = ""
    return {"message": "PDF effac√© avec succ√®s"}

@app.get("/")
async def root():
    """Page d'accueil de l'API"""
    return {
        "message": "API Question-R√©ponse sur PDF avec Google Gemini - VERSION ROBUSTE",
        "version": "2.0",
        "features": [
            "‚úÖ Syst√®me multi-strat√©gies pour garantir une r√©ponse",
            "‚úÖ Gestion intelligente des blocages de s√©curit√©",
            "‚úÖ D√©coupage automatique en chunks pour les gros documents",
            "‚úÖ Fallback syst√©matique - Aucune erreur utilisateur",
            "‚úÖ Support de multiples mod√®les Gemini"
        ],
        "default_model": "models/gemini-2.5-flash",
        "available_models": [
            "models/gemini-2.5-flash (recommand√©, rapide et stable)",
            "models/gemini-2.5-pro (plus puissant)",
            "models/gemini-2.0-flash (alternatif)"
        ],
        "endpoints": {
            "POST /upload-pdf/": "Upload un fichier PDF",
            "POST /ask/": "Pose une question sur le PDF (body: {question: 'votre question', max_tokens: 1024, model_name: 'models/gemini-2.5-flash'})",
            "GET /status/": "V√©rifie le statut de l'API",
            "DELETE /clear/": "Efface le PDF en m√©moire",
            "GET /docs": "Documentation interactive Swagger"
        },
        "setup": {
            "1": "Installer: pip install google-generativeai PyPDF2 fastapi uvicorn python-multipart",
            "2": "Remplacer GEMINI_API_KEY dans le code par votre cl√©",
            "3": "Obtenir une cl√© GRATUITE: https://makersuite.google.com/app/apikey"
        }
    }

if __name__ == "__main__":
    print("üöÄ D√©marrage de l'API Question-R√©ponse PDF avec Google Gemini v2.0")
    print("üìö Documentation: http://localhost:8002/docs")
    print("‚ú® Version ROBUSTE avec syst√®me anti-blocage\n")
    
    if GEMINI_API_KEY == "votre_cl√©_api_gemini_ici":
        print("\n‚ö†Ô∏è  ATTENTION: GEMINI_API_KEY n'est pas configur√©e!")
        print("   Remplacez 'votre_cl√©_api_gemini_ici' par votre vraie cl√© API dans le code")
        print("   Obtenez une cl√© GRATUITE sur: https://makersuite.google.com/app/apikey\n")
    else:
        print("‚úÖ Cl√© API Gemini configur√©e")
        print("‚úÖ Syst√®me multi-strat√©gies activ√©")
        print("‚úÖ Protection anti-blocage activ√©e\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8002)